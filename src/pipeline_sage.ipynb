{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed79f1c-2767-4d8d-8988-f7d1cf29ec72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import summary\n",
    "import xarray as xr\n",
    "import yaml\n",
    "\n",
    "import Dataset\n",
    "import SAGE\n",
    "import Loss\n",
    "from utils import time_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a033a6e-d7e1-43a4-8556-ec00b1761f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.0+cu121\n",
      "Cuda available: False\n",
      "Cuda version: 12.1\n",
      "Torch geometric version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Cuda device: {torch.cuda.get_device_name()}\")\n",
    "print(f\"Cuda version: {torch.version.cuda}\")\n",
    "print(f\"Torch geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335227eb-02db-4fb9-85a6-4af3562ca297",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32eb1309-2482-472b-aac9-799c45dca7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = yaml.safe_load(open('./config/pipeline.yaml'))\n",
    "\n",
    "DATA_PATH = params['input_subset_pre_processed']\n",
    "MESH_PATH = params['input_subset_grid']\n",
    "\n",
    "TRAIN_PROP = params['train_prop']\n",
    "VAL_PROP = params['val_prop']\n",
    "TEST_PROP = params['test_prop']\n",
    "TRAIN_VAL_TEST = [TRAIN_PROP, VAL_PROP, TEST_PROP]\n",
    "\n",
    "TRAIN_BATCH_SIZE = params['train_batch_size']\n",
    "VAL_BATCH_SIZE = params['val_batch_size']\n",
    "TEST_BATCH_SIZE = params['test_batch_size']\n",
    "\n",
    "N_FEATURES = params['n_features']\n",
    "HID_CHANNELS = params['hid_channels']\n",
    "N_CLASSES = params['n_classes']\n",
    "N_LAYERS = params['n_layers']\n",
    "MODEL_CHOSEN = params['model']\n",
    "\n",
    "FINAL_ACT = None\n",
    "if params['final_act'] == \"sigmoid\":\n",
    "    FINAL_ACT = torch.sigmoid\n",
    "elif params['final_act'] == \"softmax\":\n",
    "    FINAL_ACT = torch.softmax\n",
    "elif params['final_act'] == \"linear\":\n",
    "    FINAL_ACT = torch.nn.Linear(1, 1)\n",
    "\n",
    "LOSS_OP = None\n",
    "if params['loss_op'] == \"CE\":\n",
    "    LOSS_OP = torch.nn.CrossEntropyLoss()\n",
    "elif params['loss_op'] == \"WCE\":\n",
    "    class_weights = [params['loss_weight_1'], params['loss_weight_2'], params['loss_weight_3']]\n",
    "    LOSS_OP = Loss.WeightedCrossEntropyLoss(class_weights)\n",
    "\n",
    "OPTIMIZER = None\n",
    "if params['optimizer'] == \"Adam\":\n",
    "    OPTIMIZER = torch.optim.Adam\n",
    "\n",
    "LEARN_RATE = params['learn_rate']\n",
    "\n",
    "EPOCHS = params['epochs']\n",
    "\n",
    "PLOT_SHOW = params['plot_show']\n",
    "PLOT_FOLDER = params['output_images_path']\n",
    "\n",
    "# TODO use this\n",
    "PLOT_VERTICAL = params['plot_vertical']\n",
    "\n",
    "TIMESTAMP = time_func.start_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8009f87-209a-4138-9f7d-9428780d3b3f",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dcbd3fe-b76f-49eb-9921-24a6013db352",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for train-val-test split: 4999\n",
      "    Shape of node feature matrix: torch.Size([239536, 1])\n",
      "    Shape of graph connectivity in COO format: torch.Size([2, 1432160])\n",
      "    Shape of labels: torch.Size([239536])\n",
      "  ---  Datasets creation  ---  14.254 seconds.\n"
     ]
    }
   ],
   "source": [
    "random_seed = random.randint(1, 10000)\n",
    "print(f\"Random seed for train-val-test split: {random_seed}\")\n",
    "\n",
    "timestamp = time_func.start_time()\n",
    "\n",
    "train_dataset = Dataset.EddyDataset(root=DATA_PATH, mesh_path=MESH_PATH, split='train', proportions=TRAIN_VAL_TEST, random_seed=random_seed)\n",
    "val_dataset = Dataset.EddyDataset(root=DATA_PATH, mesh_path=MESH_PATH, split='val', proportions=TRAIN_VAL_TEST, random_seed=random_seed)\n",
    "test_dataset = Dataset.EddyDataset(root=DATA_PATH, mesh_path=MESH_PATH, split='test', proportions=TRAIN_VAL_TEST, random_seed=random_seed)\n",
    "\n",
    "time_func.stop_time(timestamp, \"Datasets creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e716ee0-b006-4e55-a44f-73a6e30de341",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3214 402 401\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.len(), val_dataset.len(), test_dataset.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65efdf-cfde-4feb-8cd3-f240636c1c34",
   "metadata": {},
   "source": [
    "### Testing some parameters and orientation of graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "764f45a6-dfe7-4fa7-b892-5dbc756db97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (TRAIN_PROP+VAL_PROP+TEST_PROP) != 100:\n",
    "    raise ValueError(f\"Sum of train-val-test proportions with value {TRAIN_PROP+VAL_PROP+TEST_PROP} is different from 100\")\n",
    "\n",
    "if FINAL_ACT == None:\n",
    "    raise ValueError(f\"Parameter 'final_act' is invalid with value {params['final_act']}\")\n",
    "\n",
    "if LOSS_OP == None:\n",
    "    raise ValueError(f\"Parameter 'loss_op' is invalid with value {params['loss_op']}\")\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        LOSS_OP = LOSS_OP.cuda()\n",
    "\n",
    "if OPTIMIZER == None:\n",
    "    raise ValueError(f\"Parameter 'optimizer' is invalid with value {params['optimizer']}\")\n",
    "\n",
    "dummy_graph = train_dataset[0]\n",
    "\n",
    "if dummy_graph.num_features != N_FEATURES:\n",
    "    raise ValueError(f\"Graph num_features is different from parameter N_FEATURES: ({dummy_graph.num_features} != {N_FEATURES})\")\n",
    "\n",
    "if dummy_graph.is_directed():\n",
    "    raise ValueError(\"Graph edges are directed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b756f7-7799-4dc8-b40a-aaef3b1ce904",
   "metadata": {},
   "source": [
    "### Train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d40cf8d-1eb2-4177-9830-0fac421de7de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3214 402 401\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=6, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=6, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=6, pin_memory=True)\n",
    "\n",
    "print(len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3de1d-9864-4536-b634-e05635056e20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed6db1ea-a1d4-4793-a665-838c3df37dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphSAGEModel(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): SAGEConv(1, 25, aggr=mean)\n",
       "    (1-11): 11 x SAGEConv(25, 25, aggr=mean)\n",
       "    (12): SAGEConv(25, 3, aggr=mean)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Model = SAGE.GraphSAGEModel\n",
    "\n",
    "model = Model(\n",
    "    num_features = N_FEATURES,\n",
    "    hidden_dim = HID_CHANNELS,\n",
    "    num_classes = N_CLASSES,\n",
    "    num_layers = N_LAYERS\n",
    "    #num_nodes = dummy_graph.num_nodes   # TODO can put these in Dataset.py\n",
    "    #final_act = FINAL_ACT\n",
    ").to(DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b39dc66b-4924-4ada-862c-e75be6e678f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----------------------------+----------------+----------+\n",
      "| Layer                     | Input Shape                | Output Shape   | #Param   |\n",
      "|---------------------------+----------------------------+----------------+----------|\n",
      "| GraphSAGEModel            | [239536, 239536]           | [239536, 3]    | 14,253   |\n",
      "| ├─(conv_layers)ModuleList | --                         | --             | 14,253   |\n",
      "| │    └─(0)SAGEConv        | [239536, 1], [2, 1432160]  | [239536, 25]   | 75       |\n",
      "| │    └─(1)SAGEConv        | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(2)SAGEConv        | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(3)SAGEConv        | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(4)SAGEConv        | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(5)SAGEConv        | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(6)SAGEConv        | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(7)SAGEConv        | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(8)SAGEConv        | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(9)SAGEConv        | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(10)SAGEConv       | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(11)SAGEConv       | [239536, 25], [2, 1432160] | [239536, 25]   | 1,275    |\n",
      "| │    └─(12)SAGEConv       | [239536, 25], [2, 1432160] | [239536, 3]    | 153      |\n",
      "+---------------------------+----------------------------+----------------+----------+\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.memory_summary())\n",
    "else:\n",
    "    print(summary(model, dummy_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc09936-0b86-4ac6-83a9-014239ce3675",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18c4bc3e-ff40-44eb-8e85-21bf090eaa05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = OPTIMIZER(model.parameters(), lr=LEARN_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dba108-aab1-4308-8ef1-f9af1595b818",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba5d99c5-5c07-494b-8eb4-849741d46173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        OPTIMIZER.zero_grad()\n",
    "\n",
    "        # forward + loss\n",
    "        pred = model(batch)\n",
    "        print('train pred:', pred)\n",
    "        loss = LOSS_OP(pred, batch.y)\n",
    "        print('train loss:', loss, '\\n')\n",
    "\n",
    "        # If you try the Soft Dice Score, use this(even if the loss stays constant)\n",
    "        #loss.requires_grad = True\n",
    "        #loss = torch.tensor(loss.item(), requires_grad=True)\n",
    "\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "        \n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader.dataset)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191eccf-0b3e-4e2f-9eb8-d0b68e2f5aa8",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c067d1ee-f24b-41e5-8fd4-6de0f9546ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        # forward + loss\n",
    "        pred = model(batch)\n",
    "        print('val pred:', pred)\n",
    "        loss = LOSS_OP(pred, batch.y)\n",
    "        print('val loss:', loss, '\\n')\n",
    "\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "    \n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c0bd0-d87d-407a-a00f-c3b1baeb6075",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Computation time check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb5fb713-fda3-40bd-b729-be63ec773546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ---  Computation before training finished!  ---  243.595 seconds.\n"
     ]
    }
   ],
   "source": [
    "time_func.stop_time(TIMESTAMP, \"Computation before training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e39fb6f-6f39-4071-9271-ae684ba04d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom time import time\\nimport multiprocessing as mp\\n\\nfor num_workers in range(2, mp.cpu_count(), 2):\\n    train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\\n    val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\\n    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\\n\\n    start = time()\\n    for epoch in range(1, 3):\\n        for i, data in enumerate(train_loader, 0):\\n            pass\\n        for i, data in enumerate(val_loader, 0):\\n            pass\\n        for i, data in enumerate(test_loader, 0):\\n            pass\\n    end = time()\\n    print(\"Finish with: {} second, num_workers={}\".format(end - start, num_workers))\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "for num_workers in range(2, mp.cpu_count(), 2):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    start = time()\n",
    "    for epoch in range(1, 3):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            pass\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            pass\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            pass\n",
    "    end = time()\n",
    "    print(\"Finish with: {} second, num_workers={}\".format(end - start, num_workers))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8995f5-2bb4-4fee-bc5d-523333087f5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Epoch training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa97935-88b2-411b-aec5-c21441b0d85e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pred: tensor([[-1.8462, -0.5696, -1.2858],\n",
      "        [-1.8462, -0.5696, -1.2858],\n",
      "        [-1.8462, -0.5696, -1.2858],\n",
      "        ...,\n",
      "        [-1.8464, -0.5698, -1.2853],\n",
      "        [-1.8464, -0.5698, -1.2853],\n",
      "        [-1.8464, -0.5698, -1.2853]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.3107, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.7943, -0.5714, -1.3129],\n",
      "        [-1.7943, -0.5714, -1.3129],\n",
      "        [-1.7943, -0.5714, -1.3129],\n",
      "        ...,\n",
      "        [-1.7944, -0.5713, -1.3131],\n",
      "        [-1.7944, -0.5713, -1.3131],\n",
      "        [-1.7944, -0.5713, -1.3131]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.3053, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.7429, -0.5719, -1.3450],\n",
      "        [-1.7429, -0.5719, -1.3450],\n",
      "        [-1.7429, -0.5719, -1.3450],\n",
      "        ...,\n",
      "        [-1.7427, -0.5716, -1.3459],\n",
      "        [-1.7427, -0.5716, -1.3459],\n",
      "        [-1.7427, -0.5716, -1.3459]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.2870, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.6814, -0.5730, -1.3861],\n",
      "        [-1.6814, -0.5730, -1.3861],\n",
      "        [-1.6813, -0.5730, -1.3863],\n",
      "        ...,\n",
      "        [-1.6807, -0.5725, -1.3878],\n",
      "        [-1.6807, -0.5725, -1.3878],\n",
      "        [-1.6807, -0.5725, -1.3878]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.2727, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.6111, -0.5788, -1.4282],\n",
      "        [-1.6111, -0.5788, -1.4282],\n",
      "        [-1.6110, -0.5787, -1.4285],\n",
      "        ...,\n",
      "        [-1.6099, -0.5779, -1.4312],\n",
      "        [-1.6099, -0.5779, -1.4312],\n",
      "        [-1.6100, -0.5779, -1.4311]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.2488, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.5355, -0.5908, -1.4662],\n",
      "        [-1.5355, -0.5908, -1.4662],\n",
      "        [-1.5354, -0.5908, -1.4666],\n",
      "        ...,\n",
      "        [-1.5344, -0.5901, -1.4690],\n",
      "        [-1.5344, -0.5901, -1.4690],\n",
      "        [-1.5344, -0.5901, -1.4690]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.2332, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.4513, -0.6106, -1.5019],\n",
      "        [-1.4513, -0.6106, -1.5019],\n",
      "        [-1.4514, -0.6106, -1.5018],\n",
      "        ...,\n",
      "        [-1.4499, -0.6095, -1.5061],\n",
      "        [-1.4499, -0.6095, -1.5061],\n",
      "        [-1.4499, -0.6095, -1.5061]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.2122, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.3601, -0.6383, -1.5362],\n",
      "        [-1.3601, -0.6383, -1.5363],\n",
      "        [-1.3601, -0.6383, -1.5364],\n",
      "        ...,\n",
      "        [-1.3604, -0.6386, -1.5353],\n",
      "        [-1.3604, -0.6386, -1.5353],\n",
      "        [-1.3604, -0.6386, -1.5352]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.1859, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.2804, -0.6736, -1.5501],\n",
      "        [-1.2804, -0.6736, -1.5501],\n",
      "        [-1.2804, -0.6737, -1.5500],\n",
      "        ...,\n",
      "        [-1.2804, -0.6724, -1.5531],\n",
      "        [-1.2804, -0.6724, -1.5532],\n",
      "        [-1.2804, -0.6724, -1.5532]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.1662, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.2057, -0.7160, -1.5520],\n",
      "        [-1.2057, -0.7160, -1.5520],\n",
      "        [-1.2057, -0.7161, -1.5518],\n",
      "        ...,\n",
      "        [-1.2055, -0.7134, -1.5585],\n",
      "        [-1.2055, -0.7134, -1.5585],\n",
      "        [-1.2055, -0.7134, -1.5584]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.1420, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1271, -0.7658, -1.5557],\n",
      "        [-1.1271, -0.7658, -1.5556],\n",
      "        [-1.1271, -0.7657, -1.5557],\n",
      "        ...,\n",
      "        [-1.1273, -0.7640, -1.5592],\n",
      "        [-1.1273, -0.7640, -1.5593],\n",
      "        [-1.1273, -0.7640, -1.5593]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.1229, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0540, -0.8221, -1.5515],\n",
      "        [-1.0540, -0.8221, -1.5514],\n",
      "        [-1.0540, -0.8225, -1.5506],\n",
      "        ...,\n",
      "        [-1.0540, -0.8215, -1.5527],\n",
      "        [-1.0540, -0.8215, -1.5528],\n",
      "        [-1.0540, -0.8214, -1.5528]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.1072, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.9861, -0.8865, -1.5377],\n",
      "        [-0.9861, -0.8865, -1.5377],\n",
      "        [-0.9861, -0.8865, -1.5377],\n",
      "        ...,\n",
      "        [-0.9856, -0.8855, -1.5405],\n",
      "        [-0.9856, -0.8855, -1.5405],\n",
      "        [-0.9856, -0.8855, -1.5406]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0856, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.9213, -0.9562, -1.5249],\n",
      "        [-0.9213, -0.9562, -1.5249],\n",
      "        [-0.9213, -0.9562, -1.5249],\n",
      "        ...,\n",
      "        [-0.9206, -0.9553, -1.5279],\n",
      "        [-0.9206, -0.9553, -1.5279],\n",
      "        [-0.9206, -0.9553, -1.5278]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0838, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.8637, -1.0299, -1.5080],\n",
      "        [-0.8637, -1.0299, -1.5080],\n",
      "        [-0.8637, -1.0299, -1.5080],\n",
      "        ...,\n",
      "        [-0.8627, -1.0282, -1.5127],\n",
      "        [-0.8627, -1.0282, -1.5127],\n",
      "        [-0.8627, -1.0282, -1.5126]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0756, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.8164, -1.1029, -1.4867],\n",
      "        [-0.8164, -1.1030, -1.4867],\n",
      "        [-0.8164, -1.1030, -1.4867],\n",
      "        ...,\n",
      "        [-0.8147, -1.1002, -1.4941],\n",
      "        [-0.8147, -1.1003, -1.4940],\n",
      "        [-0.8147, -1.1003, -1.4940]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0650, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7770, -1.1696, -1.4707],\n",
      "        [-0.7772, -1.1697, -1.4703],\n",
      "        [-0.7779, -1.1699, -1.4687],\n",
      "        ...,\n",
      "        [-0.7749, -1.1680, -1.4773],\n",
      "        [-0.7749, -1.1679, -1.4774],\n",
      "        [-0.7749, -1.1679, -1.4774]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0666, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7472, -1.2326, -1.4491],\n",
      "        [-0.7472, -1.2326, -1.4492],\n",
      "        [-0.7468, -1.2324, -1.4502],\n",
      "        ...,\n",
      "        [-0.7437, -1.2266, -1.4637],\n",
      "        [-0.7437, -1.2266, -1.4637],\n",
      "        [-0.7437, -1.2268, -1.4636]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0685, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7251, -1.2845, -1.4315],\n",
      "        [-0.7252, -1.2846, -1.4314],\n",
      "        [-0.7253, -1.2847, -1.4310],\n",
      "        ...,\n",
      "        [-0.7225, -1.2739, -1.4493],\n",
      "        [-0.7225, -1.2740, -1.4492],\n",
      "        [-0.7225, -1.2740, -1.4492]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0670, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7102, -1.3231, -1.4182],\n",
      "        [-0.7101, -1.3230, -1.4184],\n",
      "        [-0.7093, -1.3216, -1.4216],\n",
      "        ...,\n",
      "        [-0.7082, -1.3108, -1.4359],\n",
      "        [-0.7082, -1.3109, -1.4358],\n",
      "        [-0.7083, -1.3114, -1.4353]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0679, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7067, -1.3620, -1.3839],\n",
      "        [-0.7068, -1.3621, -1.3838],\n",
      "        [-0.7069, -1.3622, -1.3834],\n",
      "        ...,\n",
      "        [-0.6997, -1.3398, -1.4215],\n",
      "        [-0.6997, -1.3399, -1.4214],\n",
      "        [-0.6997, -1.3401, -1.4212]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0654, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7035, -1.3841, -1.3680],\n",
      "        [-0.7034, -1.3839, -1.3685],\n",
      "        [-0.7023, -1.3820, -1.3725],\n",
      "        ...,\n",
      "        [-0.6951, -1.3587, -1.4107],\n",
      "        [-0.6951, -1.3588, -1.4106],\n",
      "        [-0.6951, -1.3589, -1.4105]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0628, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7040, -1.3997, -1.3520],\n",
      "        [-0.7040, -1.3998, -1.3519],\n",
      "        [-0.7044, -1.4003, -1.3505],\n",
      "        ...,\n",
      "        [-0.6934, -1.3700, -1.4022],\n",
      "        [-0.6934, -1.3701, -1.4022],\n",
      "        [-0.6934, -1.3702, -1.4021]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0682, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.6955, -1.3799, -1.3880],\n",
      "        [-0.6955, -1.3801, -1.3877],\n",
      "        [-0.6956, -1.3803, -1.3875],\n",
      "        ...,\n",
      "        [-0.6942, -1.3712, -1.3995],\n",
      "        [-0.6942, -1.3712, -1.3995],\n",
      "        [-0.6942, -1.3713, -1.3994]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0592, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7171, -1.4173, -1.3113],\n",
      "        [-0.7169, -1.4170, -1.3119],\n",
      "        [-0.7151, -1.4144, -1.3176],\n",
      "        ...,\n",
      "        [-0.6976, -1.3721, -1.3917],\n",
      "        [-0.6976, -1.3723, -1.3915],\n",
      "        [-0.6976, -1.3722, -1.3916]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0657, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7177, -1.4062, -1.3203],\n",
      "        [-0.7184, -1.4071, -1.3181],\n",
      "        [-0.7214, -1.4111, -1.3092],\n",
      "        ...,\n",
      "        [-0.7027, -1.3644, -1.3893],\n",
      "        [-0.7028, -1.3647, -1.3891],\n",
      "        [-0.7028, -1.3648, -1.3889]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0653, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7396, -1.4167, -1.2724],\n",
      "        [-0.7396, -1.4167, -1.2723],\n",
      "        [-0.7397, -1.4168, -1.2721],\n",
      "        ...,\n",
      "        [-0.7106, -1.3560, -1.3826],\n",
      "        [-0.7106, -1.3562, -1.3823],\n",
      "        [-0.7106, -1.3563, -1.3822]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0676, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7206, -1.3479, -1.3714],\n",
      "        [-0.7205, -1.3450, -1.3746],\n",
      "        [-0.7200, -1.3303, -1.3910],\n",
      "        ...,\n",
      "        [-0.7202, -1.3407, -1.3796],\n",
      "        [-0.7202, -1.3412, -1.3791],\n",
      "        [-0.7202, -1.3420, -1.3783]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0576, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7651, -1.3953, -1.2484],\n",
      "        [-0.7642, -1.3941, -1.2509],\n",
      "        [-0.7579, -1.3863, -1.2682],\n",
      "        ...,\n",
      "        [-0.7313, -1.3304, -1.3692],\n",
      "        [-0.7313, -1.3307, -1.3688],\n",
      "        [-0.7313, -1.3310, -1.3685]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0527, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7653, -1.3625, -1.2773],\n",
      "        [-0.7646, -1.3617, -1.2793],\n",
      "        [-0.7627, -1.3590, -1.2850],\n",
      "        ...,\n",
      "        [-0.7433, -1.3169, -1.3606],\n",
      "        [-0.7433, -1.3177, -1.3598],\n",
      "        [-0.7434, -1.3177, -1.3597]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0446, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7787, -1.3447, -1.2716],\n",
      "        [-0.7803, -1.3471, -1.2668],\n",
      "        [-0.7932, -1.3626, -1.2324],\n",
      "        ...,\n",
      "        [-0.7547, -1.2855, -1.3730],\n",
      "        [-0.7547, -1.2870, -1.3713],\n",
      "        [-0.7547, -1.2882, -1.3701]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0424, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.7777, -1.2972, -1.3196],\n",
      "        [-0.7781, -1.2980, -1.3181],\n",
      "        [-0.7800, -1.3024, -1.3103],\n",
      "        ...,\n",
      "        [-0.7673, -1.2708, -1.3660],\n",
      "        [-0.7674, -1.2708, -1.3659],\n",
      "        [-0.7673, -1.2718, -1.3649]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0332, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.8622, -1.3675, -1.1299],\n",
      "        [-0.8620, -1.3674, -1.1303],\n",
      "        [-0.8609, -1.3668, -1.1323],\n",
      "        ...,\n",
      "        [-0.7792, -1.2420, -1.3767],\n",
      "        [-0.7794, -1.2428, -1.3754],\n",
      "        [-0.7802, -1.2482, -1.3678]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0254, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.8924, -1.3689, -1.0908],\n",
      "        [-0.8922, -1.3687, -1.0912],\n",
      "        [-0.8903, -1.3676, -1.0944],\n",
      "        ...,\n",
      "        [-0.7926, -1.2251, -1.3720],\n",
      "        [-0.7928, -1.2257, -1.3709],\n",
      "        [-0.7930, -1.2268, -1.3693]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0158, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.9216, -1.3752, -1.0518],\n",
      "        [-0.9218, -1.3753, -1.0514],\n",
      "        [-0.9225, -1.3758, -1.0503],\n",
      "        ...,\n",
      "        [-0.8049, -1.2099, -1.3680],\n",
      "        [-0.8045, -1.2085, -1.3702],\n",
      "        [-0.8047, -1.2100, -1.3682]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0084, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.9513, -1.3879, -1.0102],\n",
      "        [-0.9514, -1.3880, -1.0100],\n",
      "        [-0.9512, -1.3879, -1.0103],\n",
      "        ...,\n",
      "        [-0.8241, -1.2183, -1.3256],\n",
      "        [-0.8249, -1.2206, -1.3217],\n",
      "        [-0.8267, -1.2247, -1.3143]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9989, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.9718, -1.4018, -0.9796],\n",
      "        [-0.9713, -1.4015, -0.9803],\n",
      "        [-0.9684, -1.3993, -0.9848],\n",
      "        ...,\n",
      "        [-0.8318, -1.2098, -1.3226],\n",
      "        [-0.8325, -1.2119, -1.3190],\n",
      "        [-0.8337, -1.2148, -1.3139]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9904, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.9920, -1.4218, -0.9471],\n",
      "        [-0.9920, -1.4218, -0.9470],\n",
      "        [-0.9918, -1.4216, -0.9474],\n",
      "        ...,\n",
      "        [-0.8265, -1.1764, -1.3704],\n",
      "        [-0.8270, -1.1779, -1.3676],\n",
      "        [-0.8273, -1.1792, -1.3655]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9797, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0235, -1.4538, -0.8990],\n",
      "        [-1.0228, -1.4533, -0.8999],\n",
      "        [-1.0172, -1.4498, -0.9069],\n",
      "        ...,\n",
      "        [-0.8218, -1.1482, -1.4141],\n",
      "        [-0.8218, -1.1489, -1.4132],\n",
      "        [-0.8226, -1.1555, -1.4032]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9678, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0679, -1.5003, -0.8365],\n",
      "        [-1.0680, -1.5003, -0.8364],\n",
      "        [-1.0676, -1.5000, -0.8369],\n",
      "        ...,\n",
      "        [-0.8204, -1.1389, -1.4290],\n",
      "        [-0.8193, -1.1307, -1.4420],\n",
      "        [-0.8187, -1.1317, -1.4416]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9579, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0845, -1.5345, -0.8066],\n",
      "        [-1.0839, -1.5341, -0.8073],\n",
      "        [-1.0805, -1.5315, -0.8111],\n",
      "        ...,\n",
      "        [-0.8209, -1.1347, -1.4336],\n",
      "        [-0.8217, -1.1395, -1.4256],\n",
      "        [-0.8225, -1.1461, -1.4155]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9446, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0096, -1.5089, -0.8807],\n",
      "        [-0.9815, -1.4795, -0.9225],\n",
      "        [-0.8218, -1.1158, -1.4580],\n",
      "        ...,\n",
      "        [-0.8173, -1.1312, -1.4450],\n",
      "        [-0.8189, -1.1409, -1.4290],\n",
      "        [-0.8231, -1.1644, -1.3911]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9322, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1055, -1.6177, -0.7537],\n",
      "        [-1.1051, -1.6167, -0.7544],\n",
      "        [-1.0983, -1.6114, -0.7615],\n",
      "        ...,\n",
      "        [-0.8298, -1.0076, -1.6156],\n",
      "        [-0.8266, -1.0157, -1.6078],\n",
      "        [-0.8209, -1.0300, -1.5947]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9199, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1272, -1.7044, -0.7049],\n",
      "        [-1.1275, -1.7050, -0.7045],\n",
      "        [-1.1291, -1.7082, -0.7022],\n",
      "        ...,\n",
      "        [-0.8362, -1.2978, -1.2259],\n",
      "        [-0.8355, -1.2963, -1.2282],\n",
      "        [-0.8356, -1.2964, -1.2279]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9058, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1367, -1.7779, -0.6730],\n",
      "        [-1.1368, -1.7780, -0.6730],\n",
      "        [-1.1362, -1.7764, -0.6739],\n",
      "        ...,\n",
      "        [-0.7930, -1.0774, -1.5749],\n",
      "        [-0.7893, -1.1006, -1.5456],\n",
      "        [-0.7877, -1.1124, -1.5308]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.8931, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1385, -1.8314, -0.6549],\n",
      "        [-1.1379, -1.8311, -0.6553],\n",
      "        [-1.1347, -1.8289, -0.6580],\n",
      "        ...,\n",
      "        [-0.9087, -0.8582, -1.7543],\n",
      "        [-0.9077, -0.8594, -1.7538],\n",
      "        [-0.9069, -0.8601, -1.7537]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.8798, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0629, -0.6995, -1.8470],\n",
      "        [-1.0549, -0.7048, -1.8480],\n",
      "        [-1.0164, -0.7331, -1.8471],\n",
      "        ...,\n",
      "        [-0.8349, -0.9490, -1.7207],\n",
      "        [-0.8234, -0.9694, -1.7051],\n",
      "        [-0.7952, -1.0222, -1.6675]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.8615, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1670, -1.9333, -0.6087],\n",
      "        [-1.1668, -1.9332, -0.6089],\n",
      "        [-1.1655, -1.9326, -0.6098],\n",
      "        ...,\n",
      "        [-0.7612, -1.5335, -1.1485],\n",
      "        [-0.7728, -1.5648, -1.1113],\n",
      "        [-0.8052, -1.6265, -1.0317]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.8528, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-0.9982, -1.9013, -0.7297],\n",
      "        [-0.9990, -1.9023, -0.7287],\n",
      "        [-0.9977, -1.9030, -0.7295],\n",
      "        ...,\n",
      "        [-0.7234, -1.1638, -1.5964],\n",
      "        [-0.7228, -1.1696, -1.5890],\n",
      "        [-0.7048, -1.2559, -1.5097]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.8395, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1617, -1.9854, -0.5983],\n",
      "        [-1.1607, -1.9854, -0.5989],\n",
      "        [-1.1531, -1.9851, -0.6033],\n",
      "        ...,\n",
      "        [-0.7531, -1.0593, -1.7015],\n",
      "        [-0.7363, -1.0952, -1.6784],\n",
      "        [-0.7110, -1.1575, -1.6369]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.8296, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1238, -2.0024, -0.6163],\n",
      "        [-1.1221, -2.0026, -0.6173],\n",
      "        [-1.1121, -2.0034, -0.6231],\n",
      "        ...,\n",
      "        [-0.6594, -1.2828, -1.5819],\n",
      "        [-0.6491, -1.3278, -1.5492],\n",
      "        [-0.6378, -1.3885, -1.5045]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.8226, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "val pred: tensor([[-1.1492, -2.0038, -0.6010],\n",
      "        [-1.1496, -2.0037, -0.6007],\n",
      "        [-1.1504, -2.0034, -0.6004],\n",
      "        ...,\n",
      "        [-0.6658, -1.1927, -1.6996],\n",
      "        [-0.6529, -1.2284, -1.6782],\n",
      "        [-0.6312, -1.2968, -1.6368]])\n",
      "val loss: tensor(0.8199) \n",
      "\n",
      "val pred: tensor([[-1.1501, -2.0035, -0.6005],\n",
      "        [-1.1503, -2.0034, -0.6004],\n",
      "        [-1.1510, -2.0032, -0.6001],\n",
      "        ...,\n",
      "        [-0.6514, -1.2327, -1.6758],\n",
      "        [-0.6454, -1.2511, -1.6640],\n",
      "        [-0.6360, -1.2803, -1.6470]])\n",
      "val loss: tensor(0.8180) \n",
      "\n",
      "val pred: tensor([[-1.1314, -2.0091, -0.6101],\n",
      "        [-1.1303, -2.0094, -0.6107],\n",
      "        [-1.1238, -2.0109, -0.6142],\n",
      "        ...,\n",
      "        [-0.6318, -1.2955, -1.6368],\n",
      "        [-0.6282, -1.3064, -1.6313],\n",
      "        [-0.6303, -1.2969, -1.6389]])\n",
      "val loss: tensor(0.8165) \n",
      "\n",
      "val pred: tensor([[-1.1337, -2.0085, -0.6088],\n",
      "        [-1.1341, -2.0084, -0.6087],\n",
      "        [-1.1349, -2.0082, -0.6082],\n",
      "        ...,\n",
      "        [-0.6819, -1.1515, -1.7250],\n",
      "        [-0.6781, -1.1610, -1.7189],\n",
      "        [-0.6834, -1.1469, -1.7289]])\n",
      "val loss: tensor(0.8202) \n",
      "\n",
      "val pred: tensor([[-0.9116, -2.0226, -0.7640],\n",
      "        [-0.9139, -2.0233, -0.7618],\n",
      "        [-0.9008, -2.0198, -0.7742],\n",
      "        ...,\n",
      "        [-0.5979, -1.4778, -1.5056],\n",
      "        [-0.6005, -1.4526, -1.5255],\n",
      "        [-0.6064, -1.4041, -1.5649]])\n",
      "val loss: tensor(0.8169) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp = time_func.start_time()\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t_loss = train()\n",
    "    v_loss = evaluate(val_loader)\n",
    "    print(f'Epoch: {epoch+1:03d}, Train running loss: {t_loss:.4f}, Val running loss: {v_loss:.4f}\\n')\n",
    "    train_loss.append(t_loss)\n",
    "    valid_loss.append(v_loss)\n",
    "\n",
    "time_func.stop_time(timestamp, \"Training Complete!\\n\")\n",
    "\n",
    "metric = evaluate(test_loader)\n",
    "print(f'Metric for test: {metric:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07469213-20da-4a74-9219-4521d40a3855",
   "metadata": {},
   "source": [
    "### Comparison plot for train/validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d123bb-cdf2-42fc-856a-cbed3dcf6c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_loss, label='Train loss')\n",
    "plt.plot(valid_loss, label='Validation loss')\n",
    "plt.legend(title=\"Loss type: \" + params['loss_op'])\n",
    "\n",
    "if PLOT_SHOW:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.savefig(PLOT_FOLDER+\"/train_val_losses_demo.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe13b1a-e3c5-475b-a213-c1bb1b7dbc8b",
   "metadata": {},
   "source": [
    "### Graphical comparison model prediction/ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e033e-9969-491a-ab5e-28800642c34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.no_grad()\n",
    "model.eval()\n",
    "batch = next(iter(test_loader))\n",
    "batch = batch.to(DEVICE)\n",
    "pred = model(batch)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c03a6e-5ec6-477c-8861-df5e4c0cba1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mesh = xr.open_dataset(MESH_PATH)\n",
    "mesh_lon = mesh.lon[mesh.nodes].values\n",
    "mesh_lat = mesh.lat[mesh.nodes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb4196-350b-4cbc-b531-5856a8e45d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "this_target = batch.y[:mesh.dims['nodes_subset']]\n",
    "this_pred = []\n",
    "for p in pred[:mesh.dims['nodes_subset']]:\n",
    "    p = p.tolist()\n",
    "    max_value = max(p)\n",
    "    max_index = p.index(max_value)\n",
    "    this_pred.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3880b4f-68e4-4e14-a327-8fa3bd407277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "im = axes[0].scatter(mesh_lon, mesh_lat, c=this_target, s=1)\n",
    "im2 = axes[1].scatter(mesh_lon, mesh_lat, c=this_pred, s=1)\n",
    "\n",
    "if PLOT_SHOW:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.savefig(PLOT_FOLDER + \"/pred_vs_ground_demo.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015415e-df6a-4d72-9239-0cdd6c971e96",
   "metadata": {},
   "source": [
    "### Accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c40334-6b9f-4b86-8220-d845cecc9fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.no_grad()\n",
    "model.eval()\n",
    "correct_pred = 0\n",
    "tot_pred = 0\n",
    "tot_background = 0\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = batch.to(DEVICE)\n",
    "    pred = model(batch)\n",
    "    tot_pred += len(pred)\n",
    "    \n",
    "    pred_values = []\n",
    "    for p in pred:\n",
    "        p = p.tolist()\n",
    "        max_value = max(p)\n",
    "        max_index = p.index(max_value)\n",
    "        pred_values.append(max_index)\n",
    "    \n",
    "    for b in batch.y:\n",
    "        if b==0:\n",
    "            tot_background += 1\n",
    "    \n",
    "    if len(pred_values) != len(batch.y):\n",
    "        raise ValueError(\"Just to be extra sure, but you should never see this error appear.\")\n",
    "    \n",
    "    for i in range(len(batch.y)):\n",
    "        if pred_values[i] == batch.y[i]:\n",
    "            correct_pred += 1\n",
    "\n",
    "print(f\"Total background cells:\\t{tot_background}\")\n",
    "print(f\"Correct predictions:\\t{correct_pred}\")\n",
    "print(f\"Total predictions:\\t{tot_pred}\")\n",
    "print(f\"GraphSAGE accuracy:\\t{correct_pred/tot_pred:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939079c5-c2db-46d9-8bec-9caa800706b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eddy-tracking",
   "language": "python",
   "name": "eddy-tracking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
