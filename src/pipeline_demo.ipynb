{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed79f1c-2767-4d8d-8988-f7d1cf29ec72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import summary\n",
    "import xarray as xr\n",
    "import yaml\n",
    "\n",
    "import Dataset\n",
    "import Models\n",
    "import Loss\n",
    "from utils import time_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a033a6e-d7e1-43a4-8556-ec00b1761f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.0+cu121\n",
      "Cuda available: False\n",
      "Cuda version: 12.1\n",
      "Torch geometric version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Cuda device: {torch.cuda.get_device_name()}\")\n",
    "print(f\"Cuda version: {torch.version.cuda}\")\n",
    "print(f\"Torch geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335227eb-02db-4fb9-85a6-4af3562ca297",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32eb1309-2482-472b-aac9-799c45dca7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = yaml.safe_load(open('./config/pipeline.yaml'))\n",
    "\n",
    "DATA_PATH = params['input_subset_pre_processed']\n",
    "MESH_PATH = params['input_subset_grid']\n",
    "\n",
    "TRAIN_PROP = params['train_prop']\n",
    "VAL_PROP = params['val_prop']\n",
    "TEST_PROP = params['test_prop']\n",
    "TRAIN_VAL_TEST = [TRAIN_PROP, VAL_PROP, TEST_PROP]\n",
    "\n",
    "TRAIN_BATCH_SIZE = params['train_batch_size']\n",
    "VAL_BATCH_SIZE = params['val_batch_size']\n",
    "TEST_BATCH_SIZE = params['test_batch_size']\n",
    "\n",
    "N_FEATURES = params['n_features']\n",
    "HID_CHANNELS = params['hid_channels']\n",
    "N_CLASSES = params['n_classes']\n",
    "N_LAYERS = params['n_layers']\n",
    "MODEL_CHOSEN = params['model']\n",
    "\n",
    "FINAL_ACT = None\n",
    "if params['final_act'] == \"sigmoid\":\n",
    "    FINAL_ACT = torch.sigmoid\n",
    "elif params['final_act'] == \"softmax\":\n",
    "    FINAL_ACT = torch.softmax\n",
    "elif params['final_act'] == \"linear\":\n",
    "    FINAL_ACT = torch.nn.Linear(1, 1)\n",
    "\n",
    "LOSS_OP = None\n",
    "if params['loss_op'] == \"CE\":\n",
    "    LOSS_OP = torch.nn.CrossEntropyLoss()\n",
    "elif params['loss_op'] == \"WCE\":\n",
    "    class_weights = [params['loss_weight_1'], params['loss_weight_2'], params['loss_weight_3']]\n",
    "    LOSS_OP = Loss.WeightedCrossEntropyLoss(class_weights)\n",
    "\n",
    "OPTIMIZER = None\n",
    "if params['optimizer'] == \"Adam\":\n",
    "    OPTIMIZER = torch.optim.Adam\n",
    "\n",
    "LEARN_RATE = params['learn_rate']\n",
    "\n",
    "EPOCHS = params['epochs']\n",
    "\n",
    "PLOT_SHOW = params['plot_show']\n",
    "PLOT_FOLDER = params['output_images_path']\n",
    "\n",
    "# TODO use this\n",
    "PLOT_VERTICAL = params['plot_vertical']\n",
    "\n",
    "TIMESTAMP = time_func.start_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8009f87-209a-4138-9f7d-9428780d3b3f",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dcbd3fe-b76f-49eb-9921-24a6013db352",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for train-val-test split: 7145\n",
      "    Shape of node feature matrix: torch.Size([239536, 1])\n",
      "    Shape of graph connectivity in COO format: torch.Size([2, 1432160])\n",
      "    Shape of labels: torch.Size([239536])\n",
      "  ---  Datasets creation  ---  0.647 seconds.\n"
     ]
    }
   ],
   "source": [
    "random_seed = random.randint(1, 10000)\n",
    "print(f\"Random seed for train-val-test split: {random_seed}\")\n",
    "\n",
    "timestamp = time_func.start_time()\n",
    "\n",
    "train_dataset = Dataset.EddyDataset(root=DATA_PATH, mesh_path=MESH_PATH, split='train', proportions=TRAIN_VAL_TEST, random_seed=random_seed)\n",
    "val_dataset = Dataset.EddyDataset(root=DATA_PATH, mesh_path=MESH_PATH, split='val', proportions=TRAIN_VAL_TEST, random_seed=random_seed)\n",
    "test_dataset = Dataset.EddyDataset(root=DATA_PATH, mesh_path=MESH_PATH, split='test', proportions=TRAIN_VAL_TEST, random_seed=random_seed)\n",
    "\n",
    "time_func.stop_time(timestamp, \"Datasets creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e716ee0-b006-4e55-a44f-73a6e30de341",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292 36 37\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.len(), val_dataset.len(), test_dataset.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65efdf-cfde-4feb-8cd3-f240636c1c34",
   "metadata": {},
   "source": [
    "### Testing some parameters and orientation of graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764f45a6-dfe7-4fa7-b892-5dbc756db97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (TRAIN_PROP+VAL_PROP+TEST_PROP) != 100:\n",
    "    raise ValueError(f\"Sum of train-val-test proportions with value {TRAIN_PROP+VAL_PROP+TEST_PROP} is different from 100\")\n",
    "\n",
    "if FINAL_ACT == None:\n",
    "    raise ValueError(f\"Parameter 'final_act' is invalid with value {params['final_act']}\")\n",
    "\n",
    "if LOSS_OP == None:\n",
    "    raise ValueError(f\"Parameter 'loss_op' is invalid with value {params['loss_op']}\")\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        LOSS_OP = LOSS_OP.cuda()\n",
    "\n",
    "if OPTIMIZER == None:\n",
    "    raise ValueError(f\"Parameter 'optimizer' is invalid with value {params['optimizer']}\")\n",
    "\n",
    "dummy_graph = train_dataset[0]\n",
    "\n",
    "if dummy_graph.num_features != N_FEATURES:\n",
    "    raise ValueError(f\"Graph num_features is different from parameter N_FEATURES: ({dummy_graph.num_features} != {N_FEATURES})\")\n",
    "\n",
    "if dummy_graph.is_directed():\n",
    "    raise ValueError(\"Graph edges are directed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b756f7-7799-4dc8-b40a-aaef3b1ce904",
   "metadata": {},
   "source": [
    "### Train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d40cf8d-1eb2-4177-9830-0fac421de7de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292 36 37\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=6, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=6, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=6, pin_memory=True)\n",
    "\n",
    "print(len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3de1d-9864-4536-b634-e05635056e20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed6db1ea-a1d4-4793-a665-838c3df37dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNModel(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): GCNConv(1, 64)\n",
       "    (1-7): 7 x GCNConv(64, 64)\n",
       "    (8): GCNConv(64, 3)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if MODEL_CHOSEN == \"GUNET\": \n",
    "    Model = Models.GUNet\n",
    "\n",
    "    model = Model(\n",
    "        in_channels = N_FEATURES,\n",
    "        hidden_channels = HID_CHANNELS,\n",
    "        out_channels = N_CLASSES,\n",
    "        num_nodes = dummy_graph.num_nodes,   # TODO can put these in Dataset.py\n",
    "        final_act = FINAL_ACT\n",
    "    ).to(DEVICE)\n",
    "elif MODEL_CHOSEN == \"GCN\":\n",
    "    Model = Models.GCNModel\n",
    "\n",
    "    model = Model(\n",
    "        in_channels = N_FEATURES,\n",
    "        hidden_channels = HID_CHANNELS,\n",
    "        out_channels = N_CLASSES,\n",
    "        num_layers = N_LAYERS,\n",
    "        num_nodes = dummy_graph.num_nodes   # TODO can put these in Dataset.py\n",
    "        #final_act = FINAL_ACT\n",
    "    ).to(DEVICE)\n",
    "else:\n",
    "    Model = Models.GraphUNetWithBN\n",
    "\n",
    "    model = Model(\n",
    "        in_channels = N_FEATURES,\n",
    "        out_channels = N_CLASSES,\n",
    "        depth = N_LAYERS\n",
    "        #num_nodes = dummy_graph.num_nodes   # TODO can put these in Dataset.py\n",
    "        #final_act = FINAL_ACT\n",
    "    ).to(DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b39dc66b-4924-4ada-862c-e75be6e678f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----------------------------+----------------+----------+\n",
      "| Layer                     | Input Shape                | Output Shape   | #Param   |\n",
      "|---------------------------+----------------------------+----------------+----------|\n",
      "| GCNModel                  | [239536, 239536]           | [239536, 3]    | 29,443   |\n",
      "| ├─(conv_layers)ModuleList | --                         | --             | 29,443   |\n",
      "| │    └─(0)GCNConv         | [239536, 1], [2, 1432160]  | [239536, 64]   | 128      |\n",
      "| │    └─(1)GCNConv         | [239536, 64], [2, 1432160] | [239536, 64]   | 4,160    |\n",
      "| │    └─(2)GCNConv         | [239536, 64], [2, 1432160] | [239536, 64]   | 4,160    |\n",
      "| │    └─(3)GCNConv         | [239536, 64], [2, 1432160] | [239536, 64]   | 4,160    |\n",
      "| │    └─(4)GCNConv         | [239536, 64], [2, 1432160] | [239536, 64]   | 4,160    |\n",
      "| │    └─(5)GCNConv         | [239536, 64], [2, 1432160] | [239536, 64]   | 4,160    |\n",
      "| │    └─(6)GCNConv         | [239536, 64], [2, 1432160] | [239536, 64]   | 4,160    |\n",
      "| │    └─(7)GCNConv         | [239536, 64], [2, 1432160] | [239536, 64]   | 4,160    |\n",
      "| │    └─(8)GCNConv         | [239536, 64], [2, 1432160] | [239536, 3]    | 195      |\n",
      "+---------------------------+----------------------------+----------------+----------+\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.memory_summary())\n",
    "else:\n",
    "    print(summary(model, dummy_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc09936-0b86-4ac6-83a9-014239ce3675",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c4bc3e-ff40-44eb-8e85-21bf090eaa05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = OPTIMIZER(model.parameters(), lr=LEARN_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dba108-aab1-4308-8ef1-f9af1595b818",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba5d99c5-5c07-494b-8eb4-849741d46173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        OPTIMIZER.zero_grad()\n",
    "\n",
    "        # forward + loss\n",
    "        pred = model(batch)\n",
    "        print('train pred:', pred)\n",
    "        loss = LOSS_OP(pred, batch.y)\n",
    "        print('train loss:', loss, '\\n')\n",
    "\n",
    "        # If you try the Soft Dice Score, use this(even if the loss stays constant)\n",
    "        #loss.requires_grad = True\n",
    "        #loss = torch.tensor(loss.item(), requires_grad=True)\n",
    "\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "        \n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader.dataset)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191eccf-0b3e-4e2f-9eb8-d0b68e2f5aa8",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c067d1ee-f24b-41e5-8fd4-6de0f9546ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        # forward + loss\n",
    "        pred = model(batch)\n",
    "        print('val pred:', pred)\n",
    "        loss = LOSS_OP(pred, batch.y)\n",
    "        print('val loss:', loss, '\\n')\n",
    "\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "    \n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c0bd0-d87d-407a-a00f-c3b1baeb6075",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Computation time check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5fb713-fda3-40bd-b729-be63ec773546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ---  Computation before training finished!  ---  11.500 seconds.\n"
     ]
    }
   ],
   "source": [
    "time_func.stop_time(TIMESTAMP, \"Computation before training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e39fb6f-6f39-4071-9271-ae684ba04d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom time import time\\nimport multiprocessing as mp\\n\\nfor num_workers in range(2, mp.cpu_count(), 2):\\n    train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\\n    val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\\n    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\\n\\n    start = time()\\n    for epoch in range(1, 3):\\n        for i, data in enumerate(train_loader, 0):\\n            pass\\n        for i, data in enumerate(val_loader, 0):\\n            pass\\n        for i, data in enumerate(test_loader, 0):\\n            pass\\n    end = time()\\n    print(\"Finish with: {} second, num_workers={}\".format(end - start, num_workers))\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "for num_workers in range(2, mp.cpu_count(), 2):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    start = time()\n",
    "    for epoch in range(1, 3):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            pass\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            pass\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            pass\n",
    "    end = time()\n",
    "    print(\"Finish with: {} second, num_workers={}\".format(end - start, num_workers))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8995f5-2bb4-4fee-bc5d-523333087f5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Epoch training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa97935-88b2-411b-aec5-c21441b0d85e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pred: tensor([[-1.0995, -1.0969, -1.0995],\n",
      "        [-1.0995, -1.0968, -1.0995],\n",
      "        [-1.0993, -1.0972, -1.0993],\n",
      "        ...,\n",
      "        [-1.0986, -1.0986, -1.0986],\n",
      "        [-1.0986, -1.0986, -1.0986],\n",
      "        [-1.0986, -1.0986, -1.0986]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0988, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0986, -1.0986, -1.0986],\n",
      "        [-1.0986, -1.0986, -1.0986],\n",
      "        [-1.0986, -1.0986, -1.0986],\n",
      "        ...,\n",
      "        [-1.0986, -1.0986, -1.0986],\n",
      "        [-1.0986, -1.0986, -1.0986],\n",
      "        [-1.0986, -1.0986, -1.0986]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0986, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0995, -1.0995, -1.0968],\n",
      "        [-1.0996, -1.0996, -1.0967],\n",
      "        [-1.0996, -1.0996, -1.0967],\n",
      "        ...,\n",
      "        [-1.0992, -1.0992, -1.0974],\n",
      "        [-1.0993, -1.0993, -1.0973],\n",
      "        [-1.0992, -1.0992, -1.0974]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0986, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0998, -1.0998, -1.0962],\n",
      "        [-1.0999, -1.0999, -1.0959],\n",
      "        [-1.1000, -1.1000, -1.0959],\n",
      "        ...,\n",
      "        [-1.0978, -1.0993, -1.0987],\n",
      "        [-1.0977, -1.0993, -1.0987],\n",
      "        [-1.0978, -1.0993, -1.0987]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0982, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0982, -1.1013, -1.0963],\n",
      "        [-1.0983, -1.1015, -1.0960],\n",
      "        [-1.0984, -1.1015, -1.0959],\n",
      "        ...,\n",
      "        [-1.0946, -1.1010, -1.1003],\n",
      "        [-1.0941, -1.1012, -1.1006],\n",
      "        [-1.0946, -1.1010, -1.1002]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0975, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "val pred: tensor([[-1.0967, -1.1032, -1.0960],\n",
      "        [-1.0967, -1.1036, -1.0956],\n",
      "        [-1.0967, -1.1036, -1.0955],\n",
      "        ...,\n",
      "        [-1.0919, -1.1030, -1.1009],\n",
      "        [-1.0911, -1.1036, -1.1012],\n",
      "        [-1.0920, -1.1031, -1.1008]])\n",
      "val loss: tensor(1.0968) \n",
      "\n",
      "Epoch: 001, Train running loss: 1.0984, Val running loss: 1.0968\n",
      "\n",
      "train pred: tensor([[-1.0950, -1.1029, -1.0980],\n",
      "        [-1.0947, -1.1033, -1.0978],\n",
      "        [-1.0949, -1.1033, -1.0977],\n",
      "        ...,\n",
      "        [-1.0916, -1.1029, -1.1014],\n",
      "        [-1.0906, -1.1034, -1.1019],\n",
      "        [-1.0916, -1.1029, -1.1014]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0968, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0953, -1.1052, -1.0954],\n",
      "        [-1.0951, -1.1058, -1.0950],\n",
      "        [-1.0948, -1.1056, -1.0954],\n",
      "        ...,\n",
      "        [-1.0885, -1.1049, -1.1025],\n",
      "        [-1.0872, -1.1056, -1.1031],\n",
      "        [-1.0885, -1.1049, -1.1025]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0960, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0945, -1.1077, -1.0937],\n",
      "        [-1.0943, -1.1085, -1.0931],\n",
      "        [-1.0943, -1.1085, -1.0932],\n",
      "        ...,\n",
      "        [-1.0849, -1.1066, -1.1045],\n",
      "        [-1.0831, -1.1075, -1.1055],\n",
      "        [-1.0849, -1.1067, -1.1044]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0952, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0879, -1.1094, -1.0987],\n",
      "        [-1.0869, -1.1103, -1.0987],\n",
      "        [-1.0863, -1.1103, -1.0994],\n",
      "        ...,\n",
      "        [-1.0813, -1.1084, -1.1064],\n",
      "        [-1.0789, -1.1095, -1.1077],\n",
      "        [-1.0813, -1.1084, -1.1063]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0942, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0900, -1.1119, -1.0940],\n",
      "        [-1.0894, -1.1131, -1.0935],\n",
      "        [-1.0886, -1.1130, -1.0944],\n",
      "        ...,\n",
      "        [-1.0775, -1.1098, -1.1088],\n",
      "        [-1.0746, -1.1111, -1.1106],\n",
      "        [-1.0775, -1.1100, -1.1087]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0932, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "val pred: tensor([[-1.0924, -1.1156, -1.0881],\n",
      "        [-1.0921, -1.1172, -1.0868],\n",
      "        [-1.0922, -1.1173, -1.0865],\n",
      "        ...,\n",
      "        [-1.0732, -1.1131, -1.1100],\n",
      "        [-1.0696, -1.1149, -1.1120],\n",
      "        [-1.0732, -1.1133, -1.1098]])\n",
      "val loss: tensor(1.0918) \n",
      "\n",
      "Epoch: 002, Train running loss: 1.0953, Val running loss: 1.0918\n",
      "\n",
      "train pred: tensor([[-1.0911, -1.1151, -1.0899],\n",
      "        [-1.0906, -1.1165, -1.0890],\n",
      "        [-1.0898, -1.1164, -1.0899],\n",
      "        ...,\n",
      "        [-1.0733, -1.1123, -1.1107],\n",
      "        [-1.0699, -1.1138, -1.1128],\n",
      "        [-1.0733, -1.1124, -1.1106]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0919, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0898, -1.1182, -1.0881],\n",
      "        [-1.0895, -1.1199, -1.0868],\n",
      "        [-1.0896, -1.1200, -1.0866],\n",
      "        ...,\n",
      "        [-1.0686, -1.1153, -1.1126],\n",
      "        [-1.0645, -1.1172, -1.1150],\n",
      "        [-1.0687, -1.1154, -1.1125]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0907, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0894, -1.1216, -1.0852],\n",
      "        [-1.0887, -1.1236, -1.0840],\n",
      "        [-1.0866, -1.1235, -1.0861],\n",
      "        ...,\n",
      "        [-1.0639, -1.1186, -1.1142],\n",
      "        [-1.0590, -1.1208, -1.1173],\n",
      "        [-1.0639, -1.1184, -1.1145]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0891, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0842, -1.1254, -1.0868],\n",
      "        [-1.0832, -1.1278, -1.0855],\n",
      "        [-1.0830, -1.1278, -1.0856],\n",
      "        ...,\n",
      "        [-1.0583, -1.1208, -1.1180],\n",
      "        [-1.0526, -1.1234, -1.1214],\n",
      "        [-1.0584, -1.1210, -1.1177]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0872, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0884, -1.1300, -1.0782],\n",
      "        [-1.0877, -1.1328, -1.0763],\n",
      "        [-1.0839, -1.1327, -1.0801],\n",
      "        ...,\n",
      "        [-1.0527, -1.1255, -1.1192],\n",
      "        [-1.0461, -1.1289, -1.1229],\n",
      "        [-1.0529, -1.1257, -1.1189]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0849, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "val pred: tensor([[-1.0944, -1.1391, -1.0638],\n",
      "        [-1.0948, -1.1430, -1.0597],\n",
      "        [-1.0952, -1.1435, -1.0589],\n",
      "        ...,\n",
      "        [-1.0459, -1.1293, -1.1228],\n",
      "        [-1.0383, -1.1332, -1.1272],\n",
      "        [-1.0462, -1.1296, -1.1222]])\n",
      "val loss: tensor(1.0830) \n",
      "\n",
      "Epoch: 003, Train running loss: 1.0891, Val running loss: 1.0830\n",
      "\n",
      "train pred: tensor([[-1.0923, -1.1371, -1.0676],\n",
      "        [-1.0925, -1.1409, -1.0640],\n",
      "        [-1.0929, -1.1412, -1.0633],\n",
      "        ...,\n",
      "        [-1.0455, -1.1276, -1.1249],\n",
      "        [-1.0378, -1.1314, -1.1295],\n",
      "        [-1.0456, -1.1280, -1.1244]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0827, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0946, -1.1450, -1.0582],\n",
      "        [-1.0948, -1.1491, -1.0542],\n",
      "        [-1.0925, -1.1473, -1.0581],\n",
      "        ...,\n",
      "        [-1.0381, -1.1329, -1.1277],\n",
      "        [-1.0291, -1.1373, -1.1332],\n",
      "        [-1.0382, -1.1330, -1.1276]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0813, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0867, -1.1478, -1.0632],\n",
      "        [-1.0858, -1.1525, -1.0598],\n",
      "        [-1.0826, -1.1523, -1.0632],\n",
      "        ...,\n",
      "        [-1.0298, -1.1382, -1.1316],\n",
      "        [-1.0195, -1.1434, -1.1379],\n",
      "        [-1.0299, -1.1382, -1.1314]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0780, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.0992, -1.1628, -1.0378],\n",
      "        [-1.1004, -1.1693, -1.0309],\n",
      "        [-1.1007, -1.1697, -1.0303],\n",
      "        ...,\n",
      "        [-1.0202, -1.1420, -1.1384],\n",
      "        [-1.0086, -1.1479, -1.1458],\n",
      "        [-1.0203, -1.1425, -1.1379]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0752, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1010, -1.1729, -1.0272],\n",
      "        [-1.1027, -1.1806, -1.0190],\n",
      "        [-1.1031, -1.1810, -1.0183],\n",
      "        ...,\n",
      "        [-1.0106, -1.1467, -1.1447],\n",
      "        [-0.9974, -1.1533, -1.1533],\n",
      "        [-1.0104, -1.1471, -1.1445]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0708, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "val pred: tensor([[-1.1101, -1.1919, -1.0028],\n",
      "        [-1.1131, -1.2020, -0.9919],\n",
      "        [-1.1142, -1.2035, -0.9897],\n",
      "        ...,\n",
      "        [-0.9977, -1.1586, -1.1478],\n",
      "        [-0.9819, -1.1676, -1.1575],\n",
      "        [-0.9984, -1.1592, -1.1464]])\n",
      "val loss: tensor(1.0664) \n",
      "\n",
      "Epoch: 004, Train running loss: 1.0782, Val running loss: 1.0664\n",
      "\n",
      "train pred: tensor([[-1.1101, -1.1919, -1.0029],\n",
      "        [-1.1133, -1.2023, -0.9915],\n",
      "        [-1.1154, -1.2050, -0.9875],\n",
      "        ...,\n",
      "        [-0.9973, -1.1575, -1.1493],\n",
      "        [-0.9817, -1.1655, -1.1598],\n",
      "        [-0.9974, -1.1570, -1.1497]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0673, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1199, -1.2145, -0.9759],\n",
      "        [-1.1245, -1.2275, -0.9620],\n",
      "        [-1.1252, -1.2286, -0.9604],\n",
      "        ...,\n",
      "        [-0.9834, -1.1644, -1.1588],\n",
      "        [-0.9657, -1.1737, -1.1710],\n",
      "        [-0.9836, -1.1641, -1.1588]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0624, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1340, -1.2449, -0.9408],\n",
      "        [-1.1404, -1.2614, -0.9237],\n",
      "        [-1.1399, -1.2609, -0.9245],\n",
      "        ...,\n",
      "        [-0.9686, -1.1779, -1.1633],\n",
      "        [-0.9482, -1.1911, -1.1754],\n",
      "        [-0.9717, -1.1811, -1.1563]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0574, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1217, -1.2443, -0.9515],\n",
      "        [-1.1268, -1.2607, -0.9353],\n",
      "        [-1.1279, -1.2620, -0.9334],\n",
      "        ...,\n",
      "        [-0.9543, -1.1938, -1.1653],\n",
      "        [-0.9301, -1.2086, -1.1814],\n",
      "        [-0.9541, -1.1932, -1.1661]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0511, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1411, -1.2854, -0.9063],\n",
      "        [-1.1488, -1.3066, -0.8862],\n",
      "        [-1.1435, -1.3006, -0.8942],\n",
      "        ...,\n",
      "        [-0.9299, -1.2012, -1.1889],\n",
      "        [-0.9028, -1.2171, -1.2090],\n",
      "        [-0.9301, -1.1996, -1.1902]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0467, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "val pred: tensor([[-1.1554, -1.3253, -0.8691],\n",
      "        [-1.1663, -1.3529, -0.8440],\n",
      "        [-1.1699, -1.3575, -0.8386],\n",
      "        ...,\n",
      "        [-0.9088, -1.2201, -1.1981],\n",
      "        [-0.8780, -1.2416, -1.2192],\n",
      "        [-0.9105, -1.2220, -1.1938]])\n",
      "val loss: tensor(1.0358) \n",
      "\n",
      "Epoch: 005, Train running loss: 1.0580, Val running loss: 1.0358\n",
      "\n",
      "train pred: tensor([[-1.0516, -1.2583, -1.0038],\n",
      "        [-1.0518, -1.2764, -0.9898],\n",
      "        [-1.0731, -1.2822, -0.9660],\n",
      "        ...,\n",
      "        [-0.9100, -1.2099, -1.2066],\n",
      "        [-0.8807, -1.2284, -1.2284],\n",
      "        [-0.9102, -1.2098, -1.2062]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0383, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1348, -1.3283, -0.8828],\n",
      "        [-1.1416, -1.3539, -0.8617],\n",
      "        [-1.1334, -1.3464, -0.8725],\n",
      "        ...,\n",
      "        [-0.8849, -1.2392, -1.2119],\n",
      "        [-0.8503, -1.2651, -1.2361],\n",
      "        [-0.8865, -1.2412, -1.2078]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0299, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1447, -1.3723, -0.8483],\n",
      "        [-1.1511, -1.4014, -0.8268],\n",
      "        [-1.1299, -1.3827, -0.8533],\n",
      "        ...,\n",
      "        [-0.8674, -1.2379, -1.2379],\n",
      "        [-0.8311, -1.2651, -1.2651],\n",
      "        [-0.8643, -1.2413, -1.2392]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0207, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.2166, -1.5049, -0.7304],\n",
      "        [-1.2393, -1.5578, -0.6935],\n",
      "        [-1.2434, -1.5639, -0.6886],\n",
      "        ...,\n",
      "        [-0.8317, -1.2840, -1.2456],\n",
      "        [-0.7885, -1.3201, -1.2787],\n",
      "        [-0.8330, -1.2859, -1.2418]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(1.0091, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.2028, -1.5267, -0.7290],\n",
      "        [-1.2241, -1.5821, -0.6923],\n",
      "        [-1.2272, -1.5869, -0.6885],\n",
      "        ...,\n",
      "        [-0.8183, -1.3389, -1.2151],\n",
      "        [-0.7744, -1.3875, -1.2403],\n",
      "        [-0.8228, -1.3411, -1.2064]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9957, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "val pred: tensor([[-1.2876, -1.6997, -0.6137],\n",
      "        [-1.3259, -1.7827, -0.5687],\n",
      "        [-1.3357, -1.7982, -0.5596],\n",
      "        ...,\n",
      "        [-0.7755, -1.3525, -1.2696],\n",
      "        [-0.7244, -1.4059, -1.3084],\n",
      "        [-0.7788, -1.3586, -1.2587]])\n",
      "val loss: tensor(0.9823) \n",
      "\n",
      "Epoch: 006, Train running loss: 1.0210, Val running loss: 0.9823\n",
      "\n",
      "train pred: tensor([[-1.3395, -1.7789, -0.5635],\n",
      "        [-1.3861, -1.8730, -0.5170],\n",
      "        [-1.3848, -1.8717, -0.5179],\n",
      "        ...,\n",
      "        [-0.7824, -1.3087, -1.3001],\n",
      "        [-0.7301, -1.3556, -1.3457],\n",
      "        [-0.7751, -1.3300, -1.2915]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9852, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.2871, -1.7599, -0.5945],\n",
      "        [-1.3268, -1.8511, -0.5488],\n",
      "        [-1.3314, -1.8583, -0.5448],\n",
      "        ...,\n",
      "        [-0.7497, -1.3422, -1.3234],\n",
      "        [-0.6944, -1.3943, -1.3758],\n",
      "        [-0.7451, -1.3584, -1.3160]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9738, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.2679, -1.8128, -0.5881],\n",
      "        [-1.2950, -1.8963, -0.5517],\n",
      "        [-1.2390, -1.8318, -0.5975],\n",
      "        ...,\n",
      "        [-0.7222, -1.3669, -1.3493],\n",
      "        [-0.6614, -1.4292, -1.4091],\n",
      "        [-0.7181, -1.3775, -1.3467]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9580, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.3173, -1.9840, -0.5199],\n",
      "        [-1.3523, -2.0876, -0.4823],\n",
      "        [-1.2812, -1.9995, -0.5329],\n",
      "        ...,\n",
      "        [-0.6805, -1.4235, -1.3753],\n",
      "        [-0.6170, -1.4935, -1.4446],\n",
      "        [-0.6796, -1.4309, -1.3700]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9463, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "train pred: tensor([[-1.1591, -1.9396, -0.6116],\n",
      "        [-1.1799, -2.0459, -0.5737],\n",
      "        [-1.1785, -2.0447, -0.5748],\n",
      "        ...,\n",
      "        [-0.6555, -1.4348, -1.4160],\n",
      "        [-0.5934, -1.5005, -1.4937],\n",
      "        [-0.6670, -1.4139, -1.4123]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9321, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "val pred: tensor([[-1.6304, -2.7771, -0.2985],\n",
      "        [-1.7481, -3.0269, -0.2518],\n",
      "        [-1.7781, -3.0761, -0.2422],\n",
      "        ...,\n",
      "        [-0.5854, -1.5912, -1.4295],\n",
      "        [-0.5111, -1.7135, -1.5145],\n",
      "        [-0.5863, -1.6112, -1.4108]])\n",
      "val loss: tensor(0.9131) \n",
      "\n",
      "Epoch: 007, Train running loss: 0.9617, Val running loss: 0.9131\n",
      "\n",
      "train pred: tensor([[-1.1722, -2.1181, -0.5620],\n",
      "        [-1.2045, -2.2550, -0.5187],\n",
      "        [-1.2250, -2.2779, -0.5046],\n",
      "        ...,\n",
      "        [-0.5996, -1.5149, -1.4648],\n",
      "        [-0.5276, -1.6106, -1.5597],\n",
      "        [-0.5959, -1.5280, -1.4612]], grad_fn=<LogSoftmaxBackward0>)\n",
      "train loss: tensor(0.9183, grad_fn=<NllLossBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp = time_func.start_time()\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t_loss = train()\n",
    "    v_loss = evaluate(val_loader)\n",
    "    print(f'Epoch: {epoch+1:03d}, Train running loss: {t_loss:.4f}, Val running loss: {v_loss:.4f}\\n')\n",
    "    train_loss.append(t_loss)\n",
    "    valid_loss.append(v_loss)\n",
    "\n",
    "time_func.stop_time(timestamp, \"Training Complete!\\n\")\n",
    "\n",
    "metric = evaluate(test_loader)\n",
    "print(f'Metric for test: {metric:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07469213-20da-4a74-9219-4521d40a3855",
   "metadata": {},
   "source": [
    "### Comparison plot for train/validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d123bb-cdf2-42fc-856a-cbed3dcf6c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_loss, label='Train loss')\n",
    "plt.plot(valid_loss, label='Validation loss')\n",
    "plt.legend(title=\"Loss type: \" + params['loss_op'])\n",
    "\n",
    "if PLOT_SHOW:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.savefig(PLOT_FOLDER+\"/train_val_losses_demo.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe13b1a-e3c5-475b-a213-c1bb1b7dbc8b",
   "metadata": {},
   "source": [
    "### Graphical comparison model prediction/ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e033e-9969-491a-ab5e-28800642c34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.no_grad()\n",
    "model.eval()\n",
    "batch = next(iter(test_loader))\n",
    "batch = batch.to(DEVICE)\n",
    "pred = model(batch)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c03a6e-5ec6-477c-8861-df5e4c0cba1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mesh = xr.open_dataset(MESH_PATH)\n",
    "mesh_lon = mesh.lon[mesh.nodes].values\n",
    "mesh_lat = mesh.lat[mesh.nodes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb4196-350b-4cbc-b531-5856a8e45d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "this_target = batch.y[:mesh.dims['nodes_subset']]\n",
    "this_pred = []\n",
    "for p in pred[:mesh.dims['nodes_subset']]:\n",
    "    p = p.tolist()\n",
    "    max_value = max(p)\n",
    "    max_index = p.index(max_value)\n",
    "    this_pred.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3880b4f-68e4-4e14-a327-8fa3bd407277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "im = axes[0].scatter(mesh_lon, mesh_lat, c=this_target, s=1)\n",
    "im2 = axes[1].scatter(mesh_lon, mesh_lat, c=this_pred, s=1)\n",
    "\n",
    "if PLOT_SHOW:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.savefig(PLOT_FOLDER + \"/pred_vs_ground_demo.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015415e-df6a-4d72-9239-0cdd6c971e96",
   "metadata": {},
   "source": [
    "### Accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c40334-6b9f-4b86-8220-d845cecc9fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.no_grad()\n",
    "model.eval()\n",
    "correct_pred = 0\n",
    "tot_pred = 0\n",
    "tot_background = 0\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = batch.to(DEVICE)\n",
    "    pred = model(batch)\n",
    "    tot_pred += len(pred)\n",
    "    \n",
    "    pred_values = []\n",
    "    for p in pred:\n",
    "        p = p.tolist()\n",
    "        max_value = max(p)\n",
    "        max_index = p.index(max_value)\n",
    "        pred_values.append(max_index)\n",
    "    \n",
    "    for b in batch.y:\n",
    "        if b==0:\n",
    "            tot_background += 1\n",
    "    \n",
    "    if len(pred_values) != len(batch.y):\n",
    "        raise ValueError(\"Just to be extra sure, but you should never see this error appear.\")\n",
    "    \n",
    "    for i in range(len(batch.y)):\n",
    "        if pred_values[i] == batch.y[i]:\n",
    "            correct_pred += 1\n",
    "\n",
    "print(f\"Total background cells:\\t{tot_background}\")\n",
    "print(f\"Correct predictions:\\t{correct_pred}\")\n",
    "print(f\"Total predictions:\\t{tot_pred}\")\n",
    "print(f\"Graph U-Net accuracy:\\t{correct_pred/tot_pred:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04629d3-e891-4bed-892d-759f71eb15a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eddy-tracking",
   "language": "python",
   "name": "eddy-tracking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
